{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c09ef90",
   "metadata": {},
   "source": [
    "# Context Engine\n",
    "\n",
    "Copyright 2025, Denis Rothman\n",
    "\n",
    "\n",
    "**Building the Context Engine**\n",
    "\n",
    "*From a Team of Agents to an Intelligent System*\n",
    "\n",
    "\n",
    "Trong các chương trước, chúng ta đã xây dựng (**engineered**) các **contexts** riêng lẻ và các **specialist agents**. Tuy nhiên, khi hệ thống phát triển, việc quản lý các **agents** này theo một trình tự tuyến tính (**linear sequence**) cố định sẽ trở nên khó khăn và cứng nhắc. Bước tiến hóa tiếp theo là tạo ra một hệ thống có khả năng tư duy, lập kế hoạch và điều phối (**orchestrate**) các **agents** này một cách linh hoạt (**dynamically**) để đạt được mục tiêu cấp cao (**high-level goal**).\n",
    "\n",
    "Notebook này sẽ giới thiệu về **Context Engine**, một bộ điều khiển thông minh (**intelligent controller**) được thiết kế để chuyển đổi một yêu cầu mơ hồ từ người dùng thành một đầu ra được tạo ra cẩn thận và có khả năng nhận biết ngữ cảnh (**context-aware**). Nó đóng vai trò là một **orchestrator**, thực hiện ủy thác trách nhiệm cho các thành phần chuyên biệt thay vì tự mình giải quyết toàn bộ tác vụ.\n",
    "\n",
    "### Đột phá then chốt: Lập kế hoạch linh hoạt dựa trên LLM (Dynamic, LLM-Powered Planning)\n",
    "\n",
    "Cải tiến thực sự trong chương này là việc loại bỏ các **hardcoded workflows** (luồng công việc được lập trình cứng). Chúng ta sẽ xây dựng một **Planner** sử dụng một **LLM** bên ngoài để phân tích mục tiêu của người dùng. Bằng cách tham chiếu một **registry** của các công cụ (**tools**) hiện có, **Planner** này sẽ tạo ra một bản kế hoạch **JSON** đa bước tùy chỉnh một cách tức thời (**on the fly**). Thiết kế mạnh mẽ này giúp tách biệt giữa \"việc cần làm\" (mục tiêu) và \"cách thực hiện\" (kế hoạch).\n",
    "\n",
    "---\n",
    "\n",
    "### Các thành phần bạn sẽ xây dựng trong Notebook này:\n",
    "\n",
    "* **The Specialist Agents:** Bao gồm `Librarian`, `Researcher`, và `Writer` từ các bài học trước, chịu trách nhiệm xử lý phong cách (**style**), dữ kiện (**facts**) và tạo nội dung (**content generation**).\n",
    "* **The Agent Registry:** Một \"bộ công cụ\" (**toolkit**) mô tả khả năng của từng **agent**, giúp chúng có thể được tìm thấy và sử dụng bởi **Planner**.\n",
    "* **The Engine's \"Brain\"**: Bộ điều phối cốt lõi, bao gồm:\n",
    "* **Planner**: Tạo ra kế hoạch chiến lược.\n",
    "* **Executor**: Thực thi kế hoạch và quản lý **Context Chaining**, nơi đầu ra của một **agent** sẽ trở thành đầu vào của **agent** tiếp theo một cách liền mạch.\n",
    "* **Tracer**: Ghi lại toàn bộ quá trình (**logging**) để phục vụ mục đích minh bạch và gỡ lỗi (**debugging**).\n",
    "\n",
    "![](../images/image_04.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a234201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI client initialized.\n"
     ]
    }
   ],
   "source": [
    "# Imports and API Key Setup\n",
    "# We will use the OpenAI library to interact with the LLM and Google Colab's\n",
    "# secret manager to securely access your API key.\n",
    "\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# The client will automatically read the OPENAI_API_KEY from your environment.\n",
    "client = OpenAI()\n",
    "print(\"OpenAI client initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "daacb426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "EMBEDDING_DIM = 1536 # Dimension for text-embedding-3-small\n",
    "GENERATION_MODEL = \"gpt-5.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1a2c869",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dino/Documents/learning-2026/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Imports for this notebook\n",
    "import json\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "import tiktoken\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
    "# general imports required in the notebooks of this book\n",
    "import re\n",
    "import textwrap\n",
    "from IPython.display import display, Markdown\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c13a6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 'genai-mas-mcp-ch3' already exists.\n",
      "Clearing namespaces for a fresh start...\n"
     ]
    }
   ],
   "source": [
    "# 2.Initialize Clients\n",
    "# --- Initialize Clients (assuming this is already done) ---\n",
    "\n",
    "# --- Initialize Pinecone Client ---\n",
    "PINECONE_API_KEY = os.environ.get('PINECONE_API_KEY')\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "# --- Define Index and Namespaces (assuming this is already done) ---\n",
    "INDEX_NAME = 'genai-mas-mcp-ch3'\n",
    "NAMESPACE_KNOWLEDGE = \"KnowledgeStore\"\n",
    "NAMESPACE_CONTEXT = \"ContextLibrary\"\n",
    "spec = ServerlessSpec(cloud='aws', region='us-east-1')\n",
    "\n",
    "# Check if index exists\n",
    "if INDEX_NAME not in pc.list_indexes().names():\n",
    "    print(f\"Index '{INDEX_NAME}' not found. Creating new serverless index...\")\n",
    "    pc.create_index(\n",
    "        name=INDEX_NAME,\n",
    "        dimension=EMBEDDING_DIM, # Make sure EMBEDDING_DIM is defined\n",
    "        metric='cosine',\n",
    "        spec=spec\n",
    "    )\n",
    "    # Wait for index to be ready\n",
    "    while not pc.describe_index(INDEX_NAME).status['ready']:\n",
    "        print(\"Waiting for index to be ready...\")\n",
    "        time.sleep(1)\n",
    "    print(\"Index created successfully. It is new and empty.\")\n",
    "else:\n",
    "    # This block runs ONLY if the index already existed.\n",
    "    print(f\"Index '{INDEX_NAME}' already exists.\")\n",
    "    print(\"Clearing namespaces for a fresh start...\")\n",
    "\n",
    "    # Connect to the index to perform operations\n",
    "    index = pc.Index(INDEX_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72cbdd4",
   "metadata": {},
   "source": [
    "# 3.Helper Functions (LLM, Embeddings, and MCP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93e6a989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined.\n"
     ]
    }
   ],
   "source": [
    "#3. Helper Functions (LLM, Embeddings, MCP, Pinecone)\n",
    "# -------------------------------------------------------------------------\n",
    "# Utility functions to standardize interactions.\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "# === LLM Interaction ===\n",
    "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
    "def call_llm_robust(system_prompt, user_prompt,json_mode=False):\n",
    "    \"\"\"A centralized function to handle all LLM interactions with retries.\"\"\"\n",
    "    try:\n",
    "        response_format = {\"type\": \"json_object\"} if json_mode else {\"type\": \"text\"}\n",
    "        response = client.chat.completions.create(\n",
    "            model=GENERATION_MODEL,\n",
    "            response_format=response_format,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling LLM: {e}\")\n",
    "        # Raise the exception so the caller can handle it or the engine can stop\n",
    "        raise e\n",
    "\n",
    "# === Embeddings ===\n",
    "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
    "def get_embedding(text):\n",
    "    \"\"\"Generates embeddings for a single text query with retries.\"\"\"\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    response = client.embeddings.create(input=[text], model=EMBEDDING_MODEL)\n",
    "    return response.data[0].embedding\n",
    "\n",
    "# === Model Context Protocol (MCP) ===\n",
    "def create_mcp_message(sender, content, metadata=None):\n",
    "    \"\"\"Creates a standardized MCP message.\"\"\"\n",
    "    return {\n",
    "        \"protocol_version\": \"2.0 (Context Engine)\",\n",
    "        \"sender\": sender,\n",
    "        \"content\": content, # The actual payload/context\n",
    "        \"metadata\": metadata or {}\n",
    "    }\n",
    "\n",
    "# === Pinecone Interaction ===\n",
    "def query_pinecone(query_text, namespace, top_k=1):\n",
    "    \"\"\"Embeds the query text and searches the specified Pinecone namespace.\"\"\"\n",
    "    try:\n",
    "        query_embedding = get_embedding(query_text)\n",
    "        response = index.query(\n",
    "            vector=query_embedding,\n",
    "            namespace=namespace,\n",
    "            top_k=top_k,\n",
    "            include_metadata=True\n",
    "        )\n",
    "        return response['matches']\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying Pinecone (Namespace: {namespace}): {e}\")\n",
    "        raise e\n",
    "\n",
    "print(\"Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fecfcc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specialist Agents defined.\n"
     ]
    }
   ],
   "source": [
    "#@title 4.The Specialist Agents (The Handlers)\n",
    "# -------------------------------------------------------------------------\n",
    "# We define the specialist agents. These are largely reused from Chapter 3,\n",
    "# but enhanced to handle more flexible inputs required for dynamic planning.\n",
    "# Agents return the raw data (string) as the MCP 'content' for simplicity.\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "# === 4.1. Context Librarian Agent (Procedural RAG) ===\n",
    "def agent_context_librarian(mcp_message):\n",
    "    \"\"\"\n",
    "    Retrieves the appropriate Semantic Blueprint from the Context Library.\n",
    "    \"\"\"\n",
    "    print(\"\\n[Librarian] Activated. Analyzing intent...\")\n",
    "    # Extract the specific input required by this agent\n",
    "    requested_intent = mcp_message['content'].get('intent_query')\n",
    "\n",
    "    if not requested_intent:\n",
    "        raise ValueError(\"Librarian requires 'intent_query' in the input content.\")\n",
    "\n",
    "    # Query Pinecone Context Namespace\n",
    "    results = query_pinecone(requested_intent, NAMESPACE_CONTEXT, top_k=1)\n",
    "\n",
    "    if results:\n",
    "        match = results[0]\n",
    "        print(f\"[Librarian] Found blueprint '{match['id']}' (Score: {match['score']:.2f})\")\n",
    "        # Retrieve the blueprint JSON string stored in metadata\n",
    "        blueprint_json = match['metadata']['blueprint_json']\n",
    "        # The output content IS the blueprint itself (as a string)\n",
    "        content = blueprint_json\n",
    "    else:\n",
    "        print(\"[Librarian] No specific blueprint found. Returning default.\")\n",
    "        # Fallback default\n",
    "        content = json.dumps({\"instruction\": \"Generate the content neutrally.\"})\n",
    "\n",
    "    return create_mcp_message(\"Librarian\", content)\n",
    "\n",
    "# === 4.2. Researcher Agent (Factual RAG) ===\n",
    "def agent_researcher(mcp_message):\n",
    "    \"\"\"\n",
    "    Retrieves and synthesizes factual information from the Knowledge Base.\n",
    "    \"\"\"\n",
    "    print(\"\\n[Researcher] Activated. Investigating topic...\")\n",
    "    # Extract the specific input required by this agent\n",
    "    topic = mcp_message['content'].get('topic_query')\n",
    "\n",
    "    if not topic:\n",
    "        raise ValueError(\"Researcher requires 'topic_query' in the input content.\")\n",
    "\n",
    "    # Query Pinecone Knowledge Namespace\n",
    "    results = query_pinecone(topic, NAMESPACE_KNOWLEDGE, top_k=3)\n",
    "\n",
    "    if not results:\n",
    "        print(\"[Researcher] No relevant information found.\")\n",
    "        # Return a string indicating no data found\n",
    "        return create_mcp_message(\"Researcher\", \"No data found on the topic.\")\n",
    "\n",
    "    # Synthesize the findings (Retrieve-and-Synthesize)\n",
    "    print(f\"[Researcher] Found {len(results)} relevant chunks. Synthesizing...\")\n",
    "    source_texts = [match['metadata']['text'] for match in results]\n",
    "\n",
    "    system_prompt = \"\"\"Bạn là một chuyên gia AI về tổng hợp nghiên cứu (research synthesis). \n",
    "Hãy tổng hợp các văn bản nguồn (source texts) đã cung cấp thành một bản tóm tắt ngắn gọn, trình bày dưới dạng danh sách (bullet-pointed summary) và bám sát chủ đề của người dùng. \n",
    "Yêu cầu:\n",
    "1. Tập trung nghiêm ngặt vào các sự kiện thực tế (facts) có trong nguồn tài liệu. \n",
    "2. Tuyệt đối không thêm thông tin bên ngoài (outside information).\"\"\"\n",
    "    user_prompt = f\"Topic: {topic}\\n\\nSources:\\n\" + \"\\n\\n---\\n\\n\".join(source_texts)\n",
    "\n",
    "    # Use a low temperature for factual synthesis\n",
    "    findings = call_llm_robust(system_prompt, user_prompt)\n",
    "\n",
    "    # The output content IS the findings (as a string)\n",
    "    return create_mcp_message(\"Researcher\", findings)\n",
    "\n",
    "# === 4.3. Writer Agent (Generation) ===\n",
    "def agent_writer(mcp_message):\n",
    "    \"\"\"\n",
    "    Combines the factual research with the semantic blueprint to generate the final output.\n",
    "    Crucially enhanced to handle either raw facts OR previous content for rewriting tasks.\n",
    "    \"\"\"\n",
    "    print(\"\\n[Writer] Activated. Applying blueprint to source material...\")\n",
    "\n",
    "    # Extract inputs.\n",
    "    blueprint_json_string = mcp_message['content'].get('blueprint')\n",
    "    # Check for 'facts' first, then 'previous_content'\n",
    "    facts = mcp_message['content'].get('facts')\n",
    "    previous_content = mcp_message['content'].get('previous_content')\n",
    "\n",
    "    if not blueprint_json_string:\n",
    "        raise ValueError(\"Writer requires 'blueprint' in the input content.\")\n",
    "\n",
    "    # Determine the source material and label for the prompt\n",
    "    if facts:\n",
    "        source_material = facts\n",
    "        source_label = \"RESEARCH FINDINGS\"\n",
    "    elif previous_content:\n",
    "        source_material = previous_content\n",
    "        source_label = \"PREVIOUS CONTENT (For Rewriting)\"\n",
    "    else:\n",
    "        raise ValueError(\"Writer requires either 'facts' or 'previous_content'.\")\n",
    "\n",
    "\n",
    "    # The Writer's System Prompt incorporates the dynamically retrieved blueprint\n",
    "    system_prompt = f\"\"\"Bạn là một chuyên gia AI về tạo nội dung (content generation).\n",
    "    Nhiệm vụ của bạn là tạo nội dung dựa trên các KẾT QUẢ NGHIÊN CỨU (RESEARCH FINDINGS) được cung cấp.\n",
    "    Quan trọng nhất, bạn PHẢI cấu trúc, định hình phong cách và thiết lập các giới hạn cho kết quả đầu ra dựa trên các quy tắc được định nghĩa trong BẢN THIẾT KẾ NGỮ NGHĨA (SEMANTIC BLUEPRINT) dưới đây.\n",
    "\n",
    "    --- SEMANTIC BLUEPRINT (JSON) ---\n",
    "    {blueprint_json_string}\n",
    "    --- END SEMANTIC BLUEPRINT ---\n",
    "\n",
    "    Adhere strictly to the blueprint's instructions, style guides, and goals. The blueprint defines HOW you write; the source material defines WHAT you write about.\n",
    "    \"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "    --- SOURCE MATERIAL ({source_label}) ---\n",
    "    {source_material}\n",
    "    --- END SOURCE MATERIAL ---\n",
    "\n",
    "    Generate the content now, following the blueprint precisely.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate the final content (slightly higher temperature for potential creativity)\n",
    "    final_output = call_llm_robust(system_prompt, user_prompt)\n",
    "\n",
    "    # The output content IS the generated text (as a string)\n",
    "    return create_mcp_message(\"Writer\", final_output)\n",
    "\n",
    "print(\"Specialist Agents defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d1d3546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent Registry initialized.\n"
     ]
    }
   ],
   "source": [
    "#@title 5.The Agent Registry (The Toolkit)\n",
    "# -------------------------------------------------------------------------\n",
    "# We formalize the \"Handler Registry\" into an AgentRegistry.\n",
    "# This catalogs agents and describes their capabilities to the Planner.\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "class AgentRegistry:\n",
    "    def __init__(self):\n",
    "        # Mapping of agent names to their corresponding functions\n",
    "        self.registry = {\n",
    "            \"Librarian\": agent_context_librarian,\n",
    "            \"Researcher\": agent_researcher,\n",
    "            \"Writer\": agent_writer,\n",
    "        }\n",
    "\n",
    "    def get_handler(self, agent_name):\n",
    "        \"\"\"Retrieves the function associated with an agent name.\"\"\"\n",
    "        handler = self.registry.get(agent_name)\n",
    "        if not handler:\n",
    "            raise ValueError(f\"Agent '{agent_name}' not found in registry.\")\n",
    "        return handler\n",
    "\n",
    "    def get_capabilities_description(self):\n",
    "        \"\"\"\n",
    "        Returns a structured description of the agents for the Planner LLM.\n",
    "        This is crucial for the Planner to understand how to use the agents.\n",
    "        \"\"\"\n",
    "        return \"\"\"\n",
    "        Các Tác nhân (Agents) hiện có và các đầu vào bắt buộc của chúng:\n",
    "\n",
    "        1. TÁC NHÂN (AGENT): Librarian\n",
    "           VAI TRÒ (ROLE): Truy xuất các Bản thiết kế Ngữ nghĩa (các hướng dẫn về phong cách/cấu trúc).\n",
    "           ĐẦU VÀO (INPUTS):\n",
    "             - \"intent_query\": (Chuỗi) Một cụm từ mô tả về phong cách hoặc định dạng mong muốn.\n",
    "           ĐẦU RA (OUTPUT): Cấu trúc bản thiết kế (chuỗi JSON).\n",
    "\n",
    "        2. TÁC NHÂN (AGENT): Researcher\n",
    "           VAI TRÒ (ROLE): Truy xuất và tổng hợp thông tin thực tế về một chủ đề.\n",
    "           ĐẦU VÀO (INPUTS):\n",
    "             - \"topic_query\": (Chuỗi) Chủ đề cần nghiên cứu.\n",
    "           ĐẦU RA (OUTPUT): Các dữ kiện đã được tổng hợp (Chuỗi).\n",
    "\n",
    "        3. TÁC NHÂN (AGENT): Writer\n",
    "           VAI TRÒ (ROLE): Tạo mới hoặc viết lại nội dung bằng cách áp dụng Bản thiết kế (Blueprint) vào tài liệu nguồn.\n",
    "           ĐẦU VÀO (INPUTS):\n",
    "             - \"blueprint\": (Chuỗi/Tham chiếu) Các hướng dẫn về phong cách (thường từ Librarian).\n",
    "             - \"facts\": (Chuỗi/Tham chiếu) Thông tin thực tế (thường từ Researcher). Sử dụng thông tin này để tạo nội dung mới.\n",
    "             - \"previous_content\": (Chuỗi/Tham chiếu) Văn bản hiện có (thường từ một bước Writer trước đó). Sử dụng thông tin này để viết lại hoặc điều chỉnh nội dung.\n",
    "           ĐẦU RA (OUTPUT): Văn bản cuối cùng được tạo ra (Chuỗi).\n",
    "        \"\"\"\n",
    "\n",
    "# Initialize the global toolkit\n",
    "AGENT_TOOLKIT = AgentRegistry()\n",
    "print(\"Agent Registry initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80332f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 6.The Context Engine (Planner, Executor, Tracer)\n",
    "# -------------------------------------------------------------------------\n",
    "# This is the core innovation of Chapter 4. It replaces the linear\n",
    "# Orchestrator with a dynamic, LLM-driven planning and execution system.\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "# === 6.1. The Tracer (Debugging Implementation) ===\n",
    "class ExecutionTrace:\n",
    "    \"\"\"Logs the entire execution flow for debugging and analysis.\"\"\"\n",
    "    def __init__(self, goal):\n",
    "        self.goal = goal\n",
    "        self.plan = None\n",
    "        self.steps = []\n",
    "        self.status = \"Initialized\"\n",
    "        self.final_output = None\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def log_plan(self, plan):\n",
    "        self.plan = plan\n",
    "\n",
    "    def log_step(self, step_num, agent, planned_input, mcp_output, resolved_input):\n",
    "        \"\"\"Logs the details of a single execution step.\"\"\"\n",
    "        self.steps.append({\n",
    "            \"step\": step_num,\n",
    "            \"agent\": agent,\n",
    "             # The raw input definitions from the plan (including $$REFS$$)\n",
    "            \"planned_input\": planned_input,\n",
    "            # Crucial for debugging: What exact context did the agent receive?\n",
    "            \"resolved_context\": resolved_input,\n",
    "            \"output\": mcp_output['content']\n",
    "        })\n",
    "\n",
    "    def finalize(self, status, final_output=None):\n",
    "        self.status = status\n",
    "        self.final_output = final_output\n",
    "        self.duration = time.time() - self.start_time\n",
    "\n",
    "    def display_trace(self):\n",
    "        \"\"\"Displays the trace in a readable format.\"\"\"\n",
    "        display(Markdown(f\"### Execution Trace\\n**Goal:** {self.goal}\\n**Status:** {self.status} (Duration: {self.duration:.2f}s)\"))\n",
    "        if self.plan:\n",
    "            # Display the raw plan JSON\n",
    "            display(Markdown(f\"#### Plan:\\n```json\\n{json.dumps(self.plan, indent=2)}\\n```\"))\n",
    "\n",
    "        display(Markdown(\"#### Execution Steps:\"))\n",
    "        for step in self.steps:\n",
    "            print(f\"--- Step {step['step']}: {step['agent']} ---\")\n",
    "            print(\"  [Planned Input]:\", step['planned_input'])\n",
    "            # print(\"  [Resolved Context]:\", textwrap.shorten(str(step['resolved_context']), width=150))\n",
    "            print(\"  [Output Snippet]:\", textwrap.shorten(str(step['output']), width=150))\n",
    "            print(\"-\" * 20)\n",
    "\n",
    "\n",
    "# === 6.2. The Planner (Strategic Analysis) ===\n",
    "def planner(goal, capabilities):\n",
    "    \"\"\"\n",
    "    Analyzes the goal and generates a structured Execution Plan using the LLM.\n",
    "    \"\"\"\n",
    "    print(\"[Engine: Planner] Analyzing goal and generating execution plan...\")\n",
    "    system_prompt = f\"\"\"\n",
    "    Bạn là bộ não chiến lược (strategic core) của Context Engine. Nhiệm vụ của bạn là phân tích mục tiêu cấp cao của người dùng và thiết lập một Kế hoạch Thực thi (Execution Plan) có cấu trúc bằng cách phối hợp các agent sẵn có.\n",
    "\n",
    "    --- NĂNG LỰC HIỆN CÓ (AVAILABLE CAPABILITIES) ---\n",
    "    {capabilities}\n",
    "    --- KẾT THÚC NĂNG LỰC ---\n",
    "\n",
    "    HƯỚNG DẪN BẮT BUỘC:\n",
    "    1. Định dạng phản hồi: Kế hoạch PHẢI là một danh sách JSON (JSON list) chứa các đối tượng, mỗi đối tượng đại diện cho một \"step\" (bước).\n",
    "    2. Context Chaining: Bạn PHẢI sử dụng cơ chế chuỗi ngữ cảnh. Nếu một bước cần dữ liệu đầu vào từ một bước trước đó, hãy tham chiếu bằng cú pháp: $$STEP_X_OUTPUT$$.\n",
    "    3. Tư duy Chiến lược: Chia nhỏ các mục tiêu phức tạp (ví dụ: quy trình viết lại tuần tự) thành các bước riêng biệt. \n",
    "    4. Độ chính xác của Input Keys: Sử dụng đúng các khóa đầu vào cho agent Writer: \n",
    "       - 'facts': Dùng cho dữ liệu thực tế/thông tin mới.\n",
    "       - 'previous_content': Dùng cho nội dung cần được chỉnh sửa hoặc viết lại.\n",
    "\n",
    "    MỤC TIÊU VÍ DỤ: \"Viết một câu chuyện hồi hộp về tàu Apollo 11.\"\n",
    "    KẾ HOẠCH VÍ DỤ (JSON LIST):\n",
    "    [\n",
    "        {{\"step\": 1, \"agent\": \"Librarian\", \"input\": {{\"intent_query\": \"suspenseful narrative blueprint\"}}}},\n",
    "        {{\"step\": 2, \"agent\": \"Researcher\", \"input\": {{\"topic_query\": \"Apollo 11 landing details\"}}}},\n",
    "        {{\"step\": 3, \"agent\": \"Writer\", \"input\": {{\"blueprint\": \"$$STEP_1_OUTPUT$$\", \"facts\": \"$$STEP_2_OUTPUT$$\"}}}}\n",
    "    ]\n",
    "\n",
    "    MỤC TIÊU VÍ DỤ: \"Viết báo cáo kỹ thuật về tàu Juno, sau đó viết lại nội dung đó theo phong cách bình dân.\"\n",
    "    KẾ HOẠCH VÍ DỤ (JSON LIST):\n",
    "    [\n",
    "        {{\"step\": 1, \"agent\": \"Librarian\", \"input\": {{\"intent_query\": \"technical report structure\"}}}},\n",
    "        {{\"step\": 2, \"agent\": \"Researcher\", \"input\": {{\"topic_query\": \"Juno mission technology\"}}}},\n",
    "        {{\"step\": 3, \"agent\": \"Writer\", \"input\": {{\"blueprint\": \"$$STEP_1_OUTPUT$$\", \"facts\": \"$$STEP_2_OUTPUT$$\"}}}},\n",
    "        {{\"step\": 4, \"agent\": \"Librarian\", \"input\": {{\"intent_query\": \"casual summary style\"}}}},\n",
    "        {{\"step\": 5, \"agent\": \"Writer\", \"input\": {{\"blueprint\": \"$$STEP_4_OUTPUT$$\", \"previous_content\": \"$$STEP_3_OUTPUT$$\"}}}}\n",
    "    ]\n",
    "\n",
    "    CHỈ phản hồi duy nhất danh sách JSON, không kèm theo lời giải thích nào khác.\n",
    "    \"\"\"\n",
    "    # Call LLM in JSON mode for reliability\n",
    "    plan_json = \"\"\n",
    "    try:\n",
    "        plan_json = call_llm_robust(system_prompt, goal, json_mode=True)\n",
    "        plan = json.loads(plan_json)\n",
    "\n",
    "        if not isinstance(plan, list):\n",
    "             # Handle cases where the LLM wraps the list in a dictionary\n",
    "             if isinstance(plan, dict):\n",
    "                 if \"plan\" in plan and isinstance(plan[\"plan\"], list):\n",
    "                     plan = plan[\"plan\"]\n",
    "                 elif \"steps\" in plan and isinstance(plan[\"steps\"], list): # <--- ADD THIS CHECK\n",
    "                     plan = plan[\"steps\"]\n",
    "                 else:\n",
    "                    raise ValueError(\"Planner returned a dict, but missing 'plan' or 'steps' key.\")\n",
    "             else:\n",
    "                raise ValueError(\"Planner did not return a valid JSON list structure.\")\n",
    "\n",
    "        print(\"[Engine: Planner] Plan generated successfully.\")\n",
    "        return plan\n",
    "    except Exception as e:\n",
    "        print(f\"[Engine: Planner] Failed to generate a valid plan. Error: {e}. Raw LLM Output: {plan_json}\")\n",
    "        raise e\n",
    "\n",
    "\n",
    "# === 6.3. The Executor (Context Assembly and Execution) ===\n",
    "\n",
    "def resolve_dependencies(input_params, state):\n",
    "    \"\"\"\n",
    "    Helper function to replace $$REF$$ placeholders with actual data from the execution state.\n",
    "    This implements Context Chaining.\n",
    "    \"\"\"\n",
    "    # Use copy.deepcopy to ensure the original plan structure is not modified\n",
    "    resolved_input = copy.deepcopy(input_params)\n",
    "\n",
    "    # Recursive function to handle potential nested structures\n",
    "    def resolve(value):\n",
    "        if isinstance(value, str) and value.startswith(\"$$\") and value.endswith(\"$$\"):\n",
    "            ref_key = value[2:-2]\n",
    "            if ref_key in state:\n",
    "                # Retrieve the actual data (string) from the previous step's output\n",
    "                print(f\"[Engine: Executor] Resolved dependency {ref_key}.\")\n",
    "                return state[ref_key]\n",
    "            else:\n",
    "                raise ValueError(f\"Dependency Error: Reference {ref_key} not found in execution state.\")\n",
    "        elif isinstance(value, dict):\n",
    "            return {k: resolve(v) for k, v in value.items()}\n",
    "        elif isinstance(value, list):\n",
    "            return [resolve(v) for v in value]\n",
    "        return value\n",
    "\n",
    "    return resolve(resolved_input)\n",
    "\n",
    "\n",
    "def context_engine(goal):\n",
    "    \"\"\"\n",
    "    The main entry point for the Context Engine. Manages Planning and Execution.\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== [Context Engine] Starting New Task ===\\nGoal: {goal}\\n\")\n",
    "    trace = ExecutionTrace(goal)\n",
    "    registry = AGENT_TOOLKIT\n",
    "\n",
    "    # Phase 1: Plan\n",
    "    try:\n",
    "        capabilities = registry.get_capabilities_description()\n",
    "        plan = planner(goal, capabilities)\n",
    "        trace.log_plan(plan)\n",
    "    except Exception as e:\n",
    "        trace.finalize(\"Failed during Planning\")\n",
    "        # Return the trace even in failure for debugging\n",
    "        return None, trace\n",
    "\n",
    "    # Phase 2: Execute\n",
    "    # State stores the raw outputs (strings) of each step: { \"STEP_X_OUTPUT\": data_string }\n",
    "    state = {}\n",
    "\n",
    "    for step in plan:\n",
    "        step_num = step.get(\"step\")\n",
    "        agent_name = step.get(\"agent\")\n",
    "        planned_input = step.get(\"input\")\n",
    "\n",
    "        print(f\"\\n[Engine: Executor] Starting Step {step_num}: {agent_name}\")\n",
    "\n",
    "        try:\n",
    "            handler = registry.get_handler(agent_name)\n",
    "\n",
    "            # Context Assembly: Resolve dependencies\n",
    "            resolved_input = resolve_dependencies(planned_input, state)\n",
    "\n",
    "            # Execute Agent via MCP\n",
    "            # Create an MCP message with the RESOLVED input for the agent\n",
    "            mcp_resolved_input = create_mcp_message(\"Engine\", resolved_input)\n",
    "            mcp_output = handler(mcp_resolved_input)\n",
    "\n",
    "            # Update State and Log Trace\n",
    "            output_data = mcp_output[\"content\"]\n",
    "\n",
    "            # Store the output data (the context itself)\n",
    "            state[f\"STEP_{step_num}_OUTPUT\"] = output_data\n",
    "            trace.log_step(step_num, agent_name, planned_input, mcp_output, resolved_input)\n",
    "            print(f\"[Engine: Executor] Step {step_num} completed.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            error_message = f\"Execution failed at step {step_num} ({agent_name}): {e}\"\n",
    "            print(f\"[Engine: Executor] ERROR: {error_message}\")\n",
    "            trace.finalize(f\"Failed at Step {step_num}\")\n",
    "            # Return the trace for debugging the failure\n",
    "            return None, trace\n",
    "\n",
    "    # Finalization\n",
    "    final_output = state.get(f\"STEP_{len(plan)}_OUTPUT\")\n",
    "    trace.finalize(\"Success\", final_output)\n",
    "    print(\"\\n=== [Context Engine] Task Complete ===\")\n",
    "\n",
    "    # Return the output of the final step AND the trace\n",
    "    return final_output, trace"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning-2026",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
